{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "positional_encoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dilyarbuzan/Transformers-for-NLP-/blob/main/positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_axRHfxJYQD"
      },
      "source": [
        "#Positional Encoding\n",
        "This is a positional encoding notebook greatly inspired by work of Denis Rothman and his book.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJ8Saf6vR9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd768f6-879b-467b-da26-fa525832e10b"
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.1.2)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGXgeOyS5qBP"
      },
      "source": [
        "# upload to the text.txt file to Google Colaboratory using the file manager "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o7EeDUUu0Sh"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore') \n",
        "\n",
        "\n",
        "dprint=1 # prints outputs if set to 1, default=0\n",
        "\n",
        "#‘text.txt’ file \n",
        "sample = open(\"text.txt\", \"r\") \n",
        "s = sample.read() \n",
        "\n",
        "# processing escape characters \n",
        "f = s.replace(\"\\n\", \" \") \n",
        "\n",
        "data = [] \n",
        "\n",
        "# sentence parsing \n",
        "for i in sent_tokenize(f): \n",
        "\ttemp = [] \n",
        "\t# tokenize the sentence into words \n",
        "\tfor j in word_tokenize(i): \n",
        "\t\ttemp.append(j.lower()) \n",
        "\tdata.append(temp) \n",
        "print(data[0])\n",
        "# Creating Skip Gram model \n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 512,window = 5, sg = 1) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMHLpD0JhFaI"
      },
      "source": [
        "word1 = \"black\"\n",
        "word2 = \"brown\"\n",
        "pos1 = 2\n",
        "pos2 = 10\n",
        "# making the word embeddings for words black and brown \n",
        "aa = model2[word1].reshape(1,512)\n",
        "ba = model2[word2].reshape(1,512)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRqMB90WlSaU"
      },
      "source": [
        "\n",
        "pea = aa.copy()\n",
        "pca = aa.copy()\n",
        "peb = ba.copy()\n",
        "pcb = ba.copy()\n",
        "\n",
        "for i in range(0, 512, 2):\n",
        "  # making the positional embedding for position 2 \n",
        "  pea[0][i] = math.sin(pos1/(1000**((2*i)/512)))\n",
        "  pca[0][i] = aa[0][i] * math.sqrt(512) + pea[0][i]\n",
        "  # making the positionakl encoding for position 2\n",
        "  pea[0][i+1] = math.cos(pos1/(1000**((2*i)/512)))\n",
        "  pca[0][i+1] = aa[0][i+1] * math.sqrt(512) + pea[0][i+1]\n",
        "\n",
        "  # making the positional embedding for position 10\n",
        "  peb[0][i] = math.sin(pos2/(1000**((2*i)/512)))\n",
        "  pcb[0][i] = ba[0][i]  * math.sqrt(512) + peb[0][i]\n",
        "  ## making the positional embedding for position 10\n",
        "  peb[0][i+1] = math.cos(pos2/(1000**((2*i)/512)))\n",
        "  pcb[0][i+1] = ba[0][i+1]  * math.sqrt(512) + peb[0][i+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXFUlEfXnMmF",
        "outputId": "974a88fe-e158-4e70-9313-30cb7e89ac8e"
      },
      "source": [
        "print(word1, word2)\n",
        "print(cosine_similarity(aa, ba))\n",
        "print(cosine_similarity(pea, peb))\n",
        "print(cosine_similarity(pca, pcb))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "black brown\n",
            "[[0.99988]]\n",
            "[[0.81419]]\n",
            "[[0.94984716]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73yqOyqdnkA8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}